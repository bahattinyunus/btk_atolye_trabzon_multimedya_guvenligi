

### 1ï¸âƒ£ Temel fikir

Destek vektÃ¶r makineleri, **veri sÄ±nÄ±flandÄ±rmak** iÃ§in kullanÄ±lan gÃ¼Ã§lÃ¼ bir makine Ã¶ÄŸrenmesi algoritmasÄ±dÄ±r. AmaÃ§, iki sÄ±nÄ±fÄ± **en iyi ÅŸekilde ayÄ±ran bir Ã§izgi veya yÃ¼zeyi** bulmaktÄ±r.

* 2 boyutlu veri iÃ§in bu bir doÄŸru (line),
* 3 boyutlu veri iÃ§in bu bir dÃ¼zlem (plane),
* Daha yÃ¼ksek boyutlarda ise bir **hiperdÃ¼zlem (hyperplane)** olur.

Ã–rnek:
Elimizde iki tÃ¼r Ã§iÃ§ek olsun: kÄ±rmÄ±zÄ± ve mavi. SVM, kÄ±rmÄ±zÄ± ve mavi Ã§iÃ§ekleri ayÄ±racak **en geniÅŸ boÅŸluÄŸu (margin) saÄŸlayan Ã§izgiyi** bulur.

---

### 2ï¸âƒ£ Margin ve Destek VektÃ¶rler

SVMâ€™in kalbi burasÄ±:

* **Margin**: SÄ±nÄ±flar arasÄ±ndaki boÅŸluk. SVM, **maksimum margin**i bulmayÄ± amaÃ§lar.
* **Destek VektÃ¶rler**: Bu boÅŸluÄŸa en yakÄ±n olan veri noktalarÄ±dÄ±r. Bu noktalar, SVMâ€™in â€œkarar sÄ±nÄ±rÄ±nÄ±â€ belirler.

Yani karar sÄ±nÄ±rÄ±nÄ± sadece bu kritik noktalar belirler, diÄŸerleri deÄŸil.

---

### 3ï¸âƒ£ Lineer ve Lineer Olmayan SVM

* **Lineer SVM**: Veri lineer olarak ayrÄ±labiliyorsa direkt bir doÄŸru/hiperdÃ¼zlem Ã§izer.
* **Lineer olmayan SVM**: Veri lineer olarak ayrÄ±lamÄ±yorsa, SVM **kernel** adÄ± verilen bir yÃ¶ntemle veriyi daha yÃ¼ksek boyuta taÅŸÄ±r ve orada lineer ayÄ±rabilir.

Ã–rnek kernelâ€™lar:

* **Polynomial kernel**: Polinomlarla dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
* **RBF (Radial Basis Function)**: KarmaÅŸÄ±k sÄ±nÄ±rlar Ã§izebilir.
* **Sigmoid kernel**: SinÃ¼s benzeri bir dÃ¶nÃ¼ÅŸÃ¼m uygular.

---

### 4ï¸âƒ£ SVMâ€™in matematiÄŸi (kÄ±saca)

SVM, bir hiperdÃ¼zlem (w \cdot x + b = 0) bulur. Burada:

* (w) â†’ hiperdÃ¼zlemin normal vektÃ¶rÃ¼ (eÄŸim),
* (b) â†’ ofset (baÅŸlangÄ±Ã§ noktasÄ±).

AmaÃ§:

[
\text{margin} = \frac{2}{||w||} \quad \text{maksimize etmek}
]

ve

[
y_i (w \cdot x_i + b) \ge 1
]

ÅŸartÄ±nÄ± saÄŸlamak. Buradaki (y_i), sÄ±nÄ±f etiketidir (+1 veya -1).

---

### 5ï¸âƒ£ Avantajlar ve Dezavantajlar

**Avantajlar:**

* KarmaÅŸÄ±k sÄ±nÄ±flandÄ±rmalarda bile gÃ¼Ã§lÃ¼dÃ¼r.
* YÃ¼ksek boyutlu veri ile iyi Ã§alÄ±ÅŸÄ±r.
* SÄ±nÄ±flar arasÄ±nda net ayrÄ±m yapar.

**Dezavantajlar:**

* BÃ¼yÃ¼k veri setlerinde yavaÅŸ olabilir.
* DoÄŸru kernel seÃ§imi gerekir, yanlÄ±ÅŸ kernel performansÄ± dÃ¼ÅŸÃ¼rÃ¼r.
* GÃ¼rÃ¼ltÃ¼lÃ¼ verilerde margin esnemesi gerekir (Soft margin SVM).

---

### 6ï¸âƒ£ KÄ±sa Ã–zet

* SVM, **verileri ayÄ±rmak iÃ§in en geniÅŸ boÅŸluÄŸu bulur**.
* Bu boÅŸluÄŸu belirleyen noktalara **destek vektÃ¶rler** denir.
* Lineer veya lineer olmayan olabilir, kernelâ€™lar kullanÄ±lÄ±r.
* Matematiksel olarak **hiperdÃ¼zlem ve margin** kavramÄ±na dayanÄ±r.


### 1ï¸âƒ£ Soft Margin SVM
GerÃ§ek hayat verisi Ã§oÄŸu zaman **mÃ¼kemmel ayrÄ±labilir deÄŸildir**. GÃ¼rÃ¼ltÃ¼, hatalÄ± etiketler veya Ã§akÄ±ÅŸan sÄ±nÄ±flar olabilir. Bu durumda **hard margin SVM** Ã§alÄ±ÅŸmaz. Ä°ÅŸte soft margin devreye giriyor:

- AmaÃ§: **Hala geniÅŸ margin bulmak**, ama bazÄ± noktalarÄ±n sÄ±nÄ±rÄ± Ã§iÄŸnemesine izin vermek.  
- Matematiksel olarak: Bir **ceza terimi \(C\)** eklenir:
  - BÃ¼yÃ¼k \(C\) â†’ SVM daha az hata toleransÄ±, margin kÃ¼Ã§Ã¼lÃ¼r.  
  - KÃ¼Ã§Ã¼k \(C\) â†’ SVM daha esnek, margin geniÅŸler ama hatalarÄ± tolere eder.  

---

### 2ï¸âƒ£ Kernel Trick
Bazen veri **2D veya 3Dâ€™de lineer olarak ayrÄ±lamaz**. Ã‡Ã¶zÃ¼m: **veriyi daha yÃ¼ksek boyuta taÅŸÄ±mak**. Ama doÄŸrudan dÃ¶nÃ¼ÅŸtÃ¼rmek pahalÄ±dÄ±r. Kernel trick burda devreye girer:

- Kernel, veri noktalarÄ± arasÄ±ndaki **iÃ§ Ã§arpÄ±mÄ± (dot product)** hesaplar ve yÃ¼ksek boyutlu uzaya geÃ§iÅŸi gizli tutar.  
- PopÃ¼ler kernelâ€™lar:
  - **Linear** â†’ Basit lineer SVM.
  - **Polynomial** â†’ Polinom tabanlÄ± dÃ¶nÃ¼ÅŸÃ¼m.
  - **RBF (Gaussian)** â†’ KarmaÅŸÄ±k, esnek sÄ±nÄ±rlar.  
  - **Sigmoid** â†’ Yapay sinir aÄŸÄ± benzeri etki.

---

### 3ï¸âƒ£ Multi-class SVM
SVM doÄŸal olarak **iki sÄ±nÄ±flÄ±**dÄ±r. Ama Ã§oÄŸu veri seti birden fazla sÄ±nÄ±f iÃ§erir. Ã‡Ã¶zÃ¼m:

1. **One-vs-Rest (OvR)**: Her sÄ±nÄ±f diÄŸerlerinden ayrÄ± bir SVM ile karÅŸÄ±laÅŸtÄ±rÄ±lÄ±r.
2. **One-vs-One (OvO)**: Her sÄ±nÄ±f Ã§ifti iÃ§in ayrÄ± SVM eÄŸitilir.

---

### 4ï¸âƒ£ SVMâ€™in AvantajlarÄ±
- **YÃ¼ksek boyutlu veride gÃ¼Ã§lÃ¼**, feature sayÄ±sÄ± Ã¶rnek sayÄ±sÄ±ndan fazla olsa bile Ã§alÄ±ÅŸÄ±r.  
- Overfitting riski dÃ¼ÅŸÃ¼ktÃ¼r (Ã¶zellikle iyi C ve kernel seÃ§ilirse).  
- KÃ¼Ã§Ã¼k veri setlerinde etkili.

---

### 5ï¸âƒ£ DezavantajlarÄ±
- BÃ¼yÃ¼k veri setlerinde **hesaplama maliyeti yÃ¼ksek**.  
- Kernel ve C parametresi yanlÄ±ÅŸ seÃ§ilirse performans dÃ¼ÅŸer.  
- GÃ¼rÃ¼ltÃ¼ ve Ã§akÄ±ÅŸan sÄ±nÄ±flarda **hard margin SVM** baÅŸarÄ±sÄ±z olur.  

---

### 6ï¸âƒ£ KullanÄ±m AlanlarÄ±
- **Metin sÄ±nÄ±flandÄ±rma** (spam filtreleme, duygu analizi)  
- **GÃ¶rÃ¼ntÃ¼ iÅŸleme** (yÃ¼z tanÄ±ma, obje tespiti)  
- **Biyoinformatik** (hastalÄ±k sÄ±nÄ±flandÄ±rma, gen veri analizi)  
- **Finansal tahminler**  

---

ğŸ’¡ Ã–zetle Part 2:  
- Soft margin ile hatalara izin veriyoruz.  
- Kernel trick ile lineer olmayan veriyi yÃ¼ksek boyutta ayÄ±rÄ±yoruz.  
- Multi-class SVM ile birden fazla sÄ±nÄ±fÄ± yÃ¶netiyoruz.  
- Parametre seÃ§imi ve kernel tipi **performansÄ±n kilit noktasÄ±dÄ±r**.  
