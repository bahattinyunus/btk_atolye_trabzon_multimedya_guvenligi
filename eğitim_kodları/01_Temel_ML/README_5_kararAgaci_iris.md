# 5_kararAgaci_iris.py - Karar AÄŸaÃ§larÄ± (Decision Tree - Iris)

## ğŸ“– Kod AÃ§Ä±klamasÄ±

Bu kod, **karar aÄŸaÃ§larÄ±** (decision trees) algoritmasÄ±nÄ± kullanarak Ã§iÃ§ek tÃ¼rlerini sÄ±nÄ±flandÄ±rÄ±r. Karar aÄŸaÃ§larÄ±, "if-then-else" kurallarÄ±yla Ã§alÄ±ÅŸan, yorumlanmasÄ± kolay bir makine Ã¶ÄŸrenmesi yÃ¶ntemidir.

---

## ğŸ¯ AmaÃ§

**Iris Ã§iÃ§eÄŸi veri seti** kullanarak, Ã§iÃ§eÄŸin fiziksel Ã¶zelliklerine bakarak **tÃ¼rÃ¼nÃ¼** tahmin etmek.

**Problem Tipi:** Ã‡oklu SÄ±nÄ±flandÄ±rma (Multi-class Classification)
- **3 SÄ±nÄ±f:**
  - Setosa
  - Versicolor
  - Virginica

---

## ğŸŒ³ Karar AÄŸacÄ± Nedir?

**GÃ¶rsel YapÄ±:**
```
                [Petal Length <= 2.5?]
                /                    \
            YES/                      \NO
            /                          \
      [SETOSA]                  [Petal Width <= 1.7?]
                                /                    \
                            YES/                      \NO
                            /                          \
                    [VERSICOLOR]                  [VIRGINICA]
```

**Ã‡alÄ±ÅŸma Prensibi:**
1. KÃ¶k dÃ¼ÄŸÃ¼mden baÅŸla
2. Her dÃ¼ÄŸÃ¼mde bir Ã¶zelliÄŸi test et
3. Sonuca gÃ¶re sola/saÄŸa ilerle
4. Yaprak dÃ¼ÄŸÃ¼me ulaÅŸ â†’ Tahmin yap

---

## ğŸ“Š Kod Ä°Ã§eriÄŸi ve AdÄ±mlar

### 1. **Veri Seti HazÄ±rlama**
```python
iris = load_iris()
X = iris.data    # 4 Ã¶zellik
y = iris.target  # 3 sÄ±nÄ±f
```

**4 Ã–zellik:**
1. **sepal length** (cm) - Ã‡anak yapraÄŸÄ± uzunluÄŸu
2. **sepal width** (cm) - Ã‡anak yapraÄŸÄ± geniÅŸliÄŸi
3. **petal length** (cm) - TaÃ§ yapraÄŸÄ± uzunluÄŸu
4. **petal width** (cm) - TaÃ§ yapraÄŸÄ± geniÅŸliÄŸi

**3 SÄ±nÄ±f:**
- 0: Setosa (50 Ã¶rnek)
- 1: Versicolor (50 Ã¶rnek)
- 2: Virginica (50 Ã¶rnek)

Dengeli veri seti!

### 2. **Veri BÃ¶lme**
```python
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)
```

- EÄŸitim: 120 Ã¶rnek (%80)
- Test: 30 Ã¶rnek (%20)
- Her sÄ±nÄ±ftan 10'ar Ã¶rnek test setinde

### 3. **Model OluÅŸturma ve EÄŸitim**
```python
model = DecisionTreeClassifier(
    criterion='entropy',    # Bilgi kazancÄ± (Information Gain)
    max_depth=3,            # Maksimum derinlik
    random_state=42
)
model.fit(X_train, y_train)
```

**Parametreler:**
- **criterion:** BÃ¶lme kriteri
  - `'gini'`: Gini safsÄ±zlÄ±ÄŸÄ± (varsayÄ±lan)
  - `'entropy'`: Bilgi kazancÄ±
- **max_depth:** AÄŸacÄ±n maksimum derinliÄŸi (overfitting kontrolÃ¼)
- **min_samples_split:** BÃ¶lme iÃ§in minimum Ã¶rnek sayÄ±sÄ±
- **min_samples_leaf:** Yaprak iÃ§in minimum Ã¶rnek sayÄ±sÄ±

### 4. **Model PerformansÄ±**
```python
accuracy = accuracy_score(y_test, y_pred)
print(f"Test DoÄŸruluÄŸu: {accuracy:.3f}")
```

**Ã‡Ä±ktÄ±:**
```
EÄŸitim DoÄŸruluÄŸu: 0.983 (%98.3)
Test DoÄŸruluÄŸu: 0.967 (%96.7)
```

MÃ¼kemmel performans! Overfitting yok (eÄŸitim ve test skorlarÄ± yakÄ±n).

### 5. **KarmaÅŸÄ±klÄ±k Matrisi**
```python
confusion = confusion_matrix(y_test, y_pred)
```

**Ã‡Ä±ktÄ±:**
```
[[10  0  0]   # Setosa: 10/10 doÄŸru âœ“
 [ 0  9  1]   # Versicolor: 9/10 doÄŸru (1 Virginica ile karÄ±ÅŸtÄ±)
 [ 0  0 10]]  # Virginica: 10/10 doÄŸru âœ“
```

**Yorumlama:**
- Setosa mÃ¼kemmel ayrÄ±lÄ±yor (kolay sÄ±nÄ±f)
- 1 Versicolor Ã¶rneÄŸi Virginica olarak yanlÄ±ÅŸ sÄ±nÄ±flandÄ±rÄ±ldÄ±
- Genel baÅŸarÄ±: 29/30 = %96.7

### 6. **SÄ±nÄ±flandÄ±rma Raporu**
```python
print(classification_report(y_test, y_pred, target_names=iris.target_names))
```

**Ã‡Ä±ktÄ±:**
```
              precision    recall  f1-score   support

      setosa       1.00      1.00      1.00        10
  versicolor       1.00      0.90      0.95        10
   virginica       0.91      1.00      0.95        10

    accuracy                           0.97        30
```

**Metrik Analizi:**
- **Setosa:** MÃ¼kemmel (1.00 her metrikte)
- **Versicolor:** Recall dÃ¼ÅŸÃ¼k (0.90) â†’ 1 Ã¶rnek kaÃ§Ä±rÄ±ldÄ±
- **Virginica:** Precision dÃ¼ÅŸÃ¼k (0.91) â†’ 1 yanlÄ±ÅŸ pozitif

### 7. **Ã–zellik Ã–nem SÄ±ralamasÄ±**
```python
feature_importance = pd.DataFrame({
    'Ã–zellik': iris.feature_names,
    'Ã–nem': model.feature_importances_
}).sort_values('Ã–nem', ascending=False)
```

**Ã‡Ä±ktÄ±:**
```
             Ã–zellik      Ã–nem
  petal length (cm)  0.579077   # EN Ã–NEMLÄ°
   petal width (cm)  0.420923
  sepal length (cm)  0.000000   # KullanÄ±lmadÄ±
   sepal width (cm)  0.000000   # KullanÄ±lmadÄ±
```

**Yorumlama:**
- Model, sadece **taÃ§ yapraÄŸÄ±** Ã¶zelliklerini kullandÄ±
- Ã‡anak yapraÄŸÄ± Ã¶zellikleri sÄ±nÄ±flandÄ±rma iÃ§in gereksiz
- Petal length en ayÄ±rt edici Ã¶zellik

### 8. **Parametre KarÅŸÄ±laÅŸtÄ±rmasÄ±**
Kod, farklÄ± parametrelerle 4 model eÄŸitir:

**Ã‡Ä±ktÄ±:**
```
1. SÄ±ÄŸ aÄŸaÃ§ (Gini):
   EÄŸitim: 0.967, Test: 0.933, DÃ¼ÄŸÃ¼m: 5

2. Orta derinlik (Gini):
   EÄŸitim: 1.000, Test: 0.933, DÃ¼ÄŸÃ¼m: 15

3. Orta derinlik (Entropi):
   EÄŸitim: 0.983, Test: 0.967, DÃ¼ÄŸÃ¼m: 9

4. SÄ±nÄ±rsÄ±z derinlik (Entropi):
   EÄŸitim: 1.000, Test: 0.933, DÃ¼ÄŸÃ¼m: 15
```

**GÃ¶zlem:**
- Model 3 (max_depth=3, entropy) **en dengeli**
- Derin aÄŸaÃ§lar (model 2, 4) overfitting yaÅŸÄ±yor
- SÄ±ÄŸ aÄŸaÃ§ (model 1) underfitting

### 9. **Overfitting Analizi**
```python
train_accuracy = model.score(X_train, y_train)
test_accuracy = model.score(X_test, y_test)
difference = abs(train_accuracy - test_accuracy)
```

**Ã‡Ä±ktÄ±:**
```
EÄŸitim DoÄŸruluÄŸu: 0.983
Test DoÄŸruluÄŸu: 0.967
Performans FarkÄ±: 0.017 (iyi!)
```

**DeÄŸerlendirme:**
- Fark < 0.05 â†’ Model iyi genelleme yapÄ±yor âœ“
- Fark > 0.10 â†’ Overfitting var âœ—

### 10. **Ã‡apraz DoÄŸrulama**
```python
cv_scores = cross_val_score(model, X, y, cv=5)
print(f"Ortalama: {cv_scores.mean():.3f} (+/- {cv_scores.std():.3f})")
```

**Ã‡Ä±ktÄ±:**
```
Skorlar: [0.967, 0.967, 0.933, 1.000, 1.000]
Ortalama: 0.973 (+/- 0.050)
```

5-fold Ã§apraz doÄŸrulama ile model tutarlÄ±lÄ±ÄŸÄ± test edildi.

### 11. **Model DetaylarÄ±**
```python
print(f"AÄŸaÃ§ derinliÄŸi: {model.get_depth()}")
print(f"Yaprak sayÄ±sÄ±: {model.get_n_leaves()}")
```

**Ã‡Ä±ktÄ±:**
```
AÄŸaÃ§ derinliÄŸi: 3
Yaprak sayÄ±sÄ±: 5
Toplam dÃ¼ÄŸÃ¼m sayÄ±sÄ±: 9
```

---

## ğŸ”‘ Ã–nemli Kavramlar

### **1. Gini SafsÄ±zlÄ±ÄŸÄ± (Gini Impurity)**
```
Gini = 1 - Î£(páµ¢Â²)
```

- páµ¢: i sÄ±nÄ±fÄ±nÄ±n oranÄ±
- 0: Saf dÃ¼ÄŸÃ¼m (tek sÄ±nÄ±f)
- 0.5: Maksimum karÄ±ÅŸÄ±klÄ±k (iki sÄ±nÄ±f eÅŸit)

**Ã–rnek:**
```
DÃ¼ÄŸÃ¼mde: 40 setosa, 0 versicolor, 0 virginica
Gini = 1 - (1Â² + 0Â² + 0Â²) = 0 â†’ SAF!
```

### **2. Entropi ve Bilgi KazancÄ±**
```
Entropy = -Î£(páµ¢ * logâ‚‚(páµ¢))
```

- DÃ¼zensizlik Ã¶lÃ§Ã¼sÃ¼
- 0: Saf dÃ¼ÄŸÃ¼m
- logâ‚‚(n): Maksimum entropi

**Bilgi KazancÄ± (Information Gain):**
```
IG = Entropy(parent) - Î£(weighted_entropy(children))
```

Model, en yÃ¼ksek bilgi kazancÄ±nÄ± saÄŸlayan Ã¶zelliÄŸi seÃ§er.

### **3. Overfitting KontrolÃ¼**

**Budama (Pruning) Teknikleri:**
- **Pre-pruning:** EÄŸitim sÄ±rasÄ±nda
  - `max_depth`: Derinlik sÄ±nÄ±rÄ±
  - `min_samples_split`: BÃ¶lme iÃ§in min Ã¶rnek
  - `min_samples_leaf`: Yaprak iÃ§in min Ã¶rnek

- **Post-pruning:** EÄŸitim sonrasÄ±
  - `cost_complexity_pruning_path()` ile

---

## ğŸ“ˆ Performans Analizi

```
Test DoÄŸruluÄŸu: %96.7
Ã‡apraz DoÄŸrulama: %97.3 (Â±5%)
```

**MÃ¼kemmel Performans Nedenleri:**
1. Veri seti kÃ¼Ã§Ã¼k ve temiz
2. SÄ±nÄ±flar iyi ayrÄ±lmÄ±ÅŸ (Ã¶zellikle Setosa)
3. Ã–zellikler ayÄ±rt edici
4. Model karmaÅŸÄ±klÄ±ÄŸÄ± uygun (max_depth=3)

---

## ğŸ“ Ã–ÄŸrenme Hedefleri

âœ… Karar aÄŸaÃ§larÄ± nasÄ±l Ã§alÄ±ÅŸÄ±r
âœ… Gini ve Entropi kriterleri
âœ… Ã–zellik Ã¶nem analizi
âœ… Overfitting nasÄ±l kontrol edilir
âœ… Hiperparametre etkisi
âœ… Ã‡apraz doÄŸrulama neden Ã¶nemli
âœ… Confusion matrix detaylÄ± yorumlama

---

## ğŸš€ NasÄ±l Ã‡alÄ±ÅŸtÄ±rÄ±lÄ±r?

```bash
python 5_kararAgaci_iris.py
```

---

## ğŸ“¦ Gerekli KÃ¼tÃ¼phaneler

```python
numpy
pandas
scikit-learn
```

---

## ğŸ” Ä°leri Seviye Ä°Ã§in

### **1. AÄŸaÃ§ GÃ¶rselleÅŸtirme**
```python
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(20, 10))
plot_tree(model,
          feature_names=iris.feature_names,
          class_names=iris.target_names,
          filled=True,
          rounded=True)
plt.show()
```

### **2. Random Forest (Ensemble)**
```python
from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
# Genellikle daha yÃ¼ksek performans
```

### **3. Ã–zellik SeÃ§imi**
```python
from sklearn.feature_selection import SelectKBest, chi2

selector = SelectKBest(chi2, k=2)  # En iyi 2 Ã¶zelliÄŸi seÃ§
X_new = selector.fit_transform(X, y)
```

### **4. Hiperparametre Optimizasyonu**
```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'max_depth': [2, 3, 4, 5],
    'criterion': ['gini', 'entropy'],
    'min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)
grid_search.fit(X_train, y_train)
print(f"En iyi parametreler: {grid_search.best_params_}")
```

---

## âš–ï¸ Avantajlar vs Dezavantajlar

### **AVANTAJLARI:**
âœ… YorumlanmasÄ± kolay (beyaz kutu)
âœ… Veri Ã¶n iÅŸleme gerektirmez
âœ… Hem sayÄ±sal hem kategorik veri
âœ… DoÄŸrusal olmayan iliÅŸkileri yakalar
âœ… Ã–zellik Ã¶nem analizi built-in

### **DEZAVANTAJLARI:**
âŒ Overfitting'e eÄŸilimli
âŒ KÃ¼Ã§Ã¼k veri deÄŸiÅŸikliÄŸine hassas
âŒ Dengesiz veride bias
âŒ EÄŸik karar sÄ±nÄ±rlarÄ±nda zayÄ±f

---

## ğŸ†š DiÄŸer Algoritmalarla KarÅŸÄ±laÅŸtÄ±rma

| Algoritma | DoÄŸruluk | Yorumlanabilirlik | HÄ±z |
|-----------|----------|-------------------|-----|
| **Karar AÄŸacÄ±** | %96.7 | â­â­â­â­â­ | HÄ±zlÄ± |
| Lojistik Reg. | ~%95 | â­â­â­â­ | Ã‡ok HÄ±zlÄ± |
| Random Forest | ~%98 | â­â­ | Orta |
| Neural Network | ~%98 | â­ | YavaÅŸ |

---

## ğŸ“Œ Notlar

- Iris veri seti ML'in "Hello World"u
- GerÃ§ek uygulamalarda Random Forest tercih edilir
- AÄŸaÃ§ gÃ¶rselleÅŸtirmesi Ã¶ÄŸretim iÃ§in Ã§ok deÄŸerli
- Medikal/yasal alanlarda yorumlanabilirlik kritik

---

## ğŸ”— Ä°lgili Konular

- **Random Forest:** Ã‡oklu karar aÄŸacÄ± ensemble'Ä±
- **Gradient Boosting:** XGBoost, LightGBM
- **CART:** Classification and Regression Trees

---

**HazÄ±rlayan:** BTK AtÃ¶lye - Multimedya GÃ¼venliÄŸi
**Tarih:** 2025
**Seviye:** Orta
