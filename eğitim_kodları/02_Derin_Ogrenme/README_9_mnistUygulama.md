# ğŸ§  MNIST Rakam TanÄ±ma ile Yapay Sinir AÄŸÄ± (ANN) Projesi

Bu proje, **Artificial Neural Network (ANN)** kullanarak **MNIST rakam veri seti** Ã¼zerinde rakam tanÄ±ma iÅŸlemi gerÃ§ekleÅŸtiren kapsamlÄ± bir deep learning uygulamasÄ±dÄ±r.

## ğŸ“‹ Ä°Ã§indekiler

- [Proje HakkÄ±nda](#proje-hakkÄ±nda)
- [Kurulum](#kurulum)
- [KullanÄ±m](#kullanÄ±m)
- [Model Mimarisi](#model-mimarisi)
- [Kod YapÄ±sÄ±](#kod-yapÄ±sÄ±)
- [SonuÃ§lar](#sonuÃ§lar)
- [Teknik Detaylar](#teknik-detaylar)
- [Ã–ÄŸrenme Ã‡Ä±ktÄ±larÄ±](#Ã¶ÄŸrenme-Ã§Ä±ktÄ±larÄ±)

## ğŸ¯ Proje HakkÄ±nda

### AmaÃ§
Bu proje, **Deep Learning**'in temellerini Ã¶ÄŸretmek ve **PyTorch** framework'Ã¼ ile basit bir yapay sinir aÄŸÄ± oluÅŸturmayÄ± amaÃ§lar. El yazÄ±sÄ± rakamlarÄ± tanÄ±yabilen bir model geliÅŸtirilir.

### Veri Seti: MNIST
- **60,000** eÄŸitim gÃ¶rÃ¼ntÃ¼sÃ¼
- **10,000** test gÃ¶rÃ¼ntÃ¼sÃ¼  
- **28x28 piksel** gri tonlama gÃ¶rÃ¼ntÃ¼ler
- **10 sÄ±nÄ±f**: 0-9 rakamlarÄ±
- **Klasik benchmark** veri seti

### Temel Ã–zellikler
- âœ… **PyTorch** tabanlÄ± implementasyon
- âœ… **Basit ANN mimarisi** (2 katman)
- âœ… **Otomatik veri indirme**
- âœ… **GÃ¶rsel sonuÃ§ analizi**
- âœ… **Batch processing** optimizasyonu
- âœ… **EÄŸitim sÃ¼reci takibi**

## ğŸ”§ Kurulum

### Gerekli KÃ¼tÃ¼phaneler
```bash
pip install torch torchvision matplotlib
```

### Sistem Gereksinimleri
- **Python**: 3.7+
- **RAM**: Minimum 4GB
- **Disk**: ~100MB (veri seti iÃ§in)
- **GPU**: Opsiyonel (CPU'da Ã§alÄ±ÅŸÄ±r)

### Dosya YapÄ±sÄ±
```
â”œâ”€â”€ 9_mnistUygulama.py           # Ana uygulama dosyasÄ±
â”œâ”€â”€ README_9_mnistUygulama.md    # Bu dÃ¶kÃ¼man
â””â”€â”€ data/                        # Otomatik oluÅŸturulur (MNIST verileri)
```

## ğŸš€ KullanÄ±m

### Basit Ã‡alÄ±ÅŸtÄ±rma
```bash
python 9_mnistUygulama.py
```

### Beklenen Ã‡Ä±ktÄ±
1. **MNIST veri seti indirme** (ilk Ã§alÄ±ÅŸtÄ±rmada)
2. **5 epoch eÄŸitim sÃ¼reci** ve kayÄ±p deÄŸerleri
3. **10 adet test gÃ¶rseli** ve tahmin sonuÃ§larÄ±
4. **GÃ¶rsel analiz** ekranlarÄ±

### Program AkÄ±ÅŸÄ±
1. **Veri yÃ¼kleme** ve dÃ¶nÃ¼ÅŸÃ¼m
2. **Model tanÄ±mlama** (ANN mimarisi)
3. **EÄŸitim dÃ¶ngÃ¼sÃ¼** (5 epoch)
4. **Test ve gÃ¶rselleÅŸtirme**

## ğŸ—ï¸ Model Mimarisi

### Artificial Neural Network (ANN)
```
Input Layer:    784 neurons (28Ã—28 pixels)
                    â†“
Hidden Layer:   128 neurons + ReLU
                    â†“
Output Layer:   10 neurons (0-9 digits)
```

### Katman DetaylarÄ±

#### 1. Input Layer (GiriÅŸ KatmanÄ±)
- **Boyut**: 784 (28Ã—28 piksel dÃ¼zleÅŸtirilmiÅŸ)
- **TÃ¼r**: Tam baÄŸlantÄ±lÄ± (Fully Connected)
- **Fonksiyon**: GÃ¶rÃ¼ntÃ¼yÃ¼ vektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼rme

#### 2. Hidden Layer (Gizli Katman)
- **Boyut**: 128 nÃ¶ron
- **Aktivasyon**: ReLU (Rectified Linear Unit)
- **Rol**: Ã–zellik Ã¶ÄŸrenme ve dÃ¶nÃ¼ÅŸÃ¼m

#### 3. Output Layer (Ã‡Ä±kÄ±ÅŸ KatmanÄ±)
- **Boyut**: 10 nÃ¶ron (her rakam iÃ§in 1)
- **Aktivasyon**: Yok (raw logits)
- **Ã‡Ä±kÄ±ÅŸ**: SÄ±nÄ±f skorlarÄ±

### Model Parametreleri
```python
# Toplam parametre sayÄ±sÄ± hesaplama
input_to_hidden = 784 Ã— 128 = 100,352
hidden_bias = 128
hidden_to_output = 128 Ã— 10 = 1,280
output_bias = 10

Toplam = 100,352 + 128 + 1,280 + 10 = 101,770 parametre
```

## ğŸ“ Kod YapÄ±sÄ±

### 1. KÃ¼tÃ¼phane Ä°mportlarÄ±
```python
import torch                    # Ana PyTorch
import torch.nn as nn          # Sinir aÄŸÄ± katmanlarÄ±  
import torch.nn.functional as F # Aktivasyon fonksiyonlarÄ±
from torchvision import datasets, transforms # Veri seti
import matplotlib.pyplot as plt # GÃ¶rselleÅŸtirme
```

### 2. Veri HazÄ±rlama
```python
# Veri dÃ¶nÃ¼ÅŸÃ¼mÃ¼ (normalizasyon)
transform = transforms.ToTensor()

# Veri setlerini yÃ¼kle
train_data = datasets.MNIST(root='data', train=True, download=True, transform=transform)
test_data = datasets.MNIST(root='data', train=False, download=True, transform=transform)
```

### 3. Veri YÃ¼kleyiciler
```python
# Batch halinde veri yÃ¼kleme
train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_data, batch_size=10, shuffle=False)
```

### 4. Model SÄ±nÄ±fÄ±
```python
class SimpleANN(nn.Module):
    def __init__(self):
        super(SimpleANN, self).__init__()
        self.fc1 = nn.Linear(28*28, 128)  # Ä°lk katman
        self.fc2 = nn.Linear(128, 10)     # Ã‡Ä±kÄ±ÅŸ katmanÄ±

    def forward(self, x):
        x = x.view(-1, 28*28)             # Flatten
        x = F.relu(self.fc1(x))           # ReLU aktivasyon
        x = self.fc2(x)                   # Ã‡Ä±kÄ±ÅŸ
        return x
```

### 5. EÄŸitim DÃ¶ngÃ¼sÃ¼
```python
for epoch in range(5):
    running_loss = 0.0
    for images, labels in train_loader:
        outputs = model(images)           # Forward pass
        loss = criterion(outputs, labels) # KayÄ±p hesaplama
        
        optimizer.zero_grad()             # Gradyan temizleme
        loss.backward()                   # Backward pass
        optimizer.step()                  # Parametre gÃ¼ncelleme
        
        running_loss += loss.item()
```

## ğŸ“Š SonuÃ§lar

### Beklenen Performans

| Metrik | DeÄŸer |
|--------|--------|
| **EÄŸitim SÃ¼resi** | ~2-5 dakika |
| **Final Loss** | ~0.2-0.4 |
| **Tahmini DoÄŸruluk** | ~95-97% |
| **Epoch SayÄ±sÄ±** | 5 |

### Tipik EÄŸitim Ã‡Ä±ktÄ±sÄ±
```
Epoch 1, Loss: 0.4523
Epoch 2, Loss: 0.2156  
Epoch 3, Loss: 0.1598
Epoch 4, Loss: 0.1234
Epoch 5, Loss: 0.0987
```

### GÃ¶rsel SonuÃ§lar
Program 10 adet test gÃ¶rseli gÃ¶sterir:
- âœ… **DoÄŸru tahminler**: YeÅŸil baÅŸlÄ±k
- âŒ **YanlÄ±ÅŸ tahminler**: KÄ±rmÄ±zÄ± baÅŸlÄ±k (nadir)
- ğŸ“Š **KarÄ±ÅŸÄ±klÄ±k**: Benzer rakamlar (6-8, 4-9)

## âš™ï¸ Teknik Detaylar

### PyTorch Ã–zellikleri

#### Tensor Ä°ÅŸlemleri
- **view(-1, 28*28)**: 2D gÃ¶rÃ¼ntÃ¼yÃ¼ 1D vektÃ¶re dÃ¼zleÅŸtirme
- **ToTensor()**: PIL/numpy array'i PyTorch tensor'e Ã§evirme
- **Normalizasyon**: [0,1] aralÄ±ÄŸÄ±na Ã¶lÃ§ekleme

#### Optimizasyon
- **Adam Optimizer**: Adaptif Ã¶ÄŸrenme oranÄ±
- **Learning Rate**: 0.001 (varsayÄ±lan)
- **CrossEntropyLoss**: Ã‡ok sÄ±nÄ±flÄ± sÄ±nÄ±flandÄ±rma loss'u

#### Memory Management
- **Batch Size**: 64 (eÄŸitim), 10 (test)
- **torch.no_grad()**: Test sÄ±rasÄ±nda gradyan hesaplamayÄ± kapatma
- **model.eval()**: Dropout vb. katmanlarÄ± devre dÄ±ÅŸÄ± bÄ±rakma

### Veri Ä°ÅŸleme Pipeline
```
Raw Image (PIL) â†’ ToTensor() â†’ Normalize â†’ Flatten â†’ Model â†’ Logits â†’ Prediction
```

## ğŸ“ Ã–ÄŸrenme Ã‡Ä±ktÄ±larÄ±

### Teorik Bilgiler
- âœ… **Artificial Neural Network** temelleri
- âœ… **PyTorch framework** kullanÄ±mÄ±
- âœ… **Backpropagation** algoritmasÄ±
- âœ… **Loss fonksiyonlarÄ±** ve optimizasyon
- âœ… **Batch processing** konsepti
- âœ… **Overfitting/Underfitting** kavramlarÄ±

### Pratik Beceriler
- âœ… **PyTorch model** tanÄ±mlama
- âœ… **MNIST veri seti** kullanÄ±mÄ±
- âœ… **EÄŸitim dÃ¶ngÃ¼sÃ¼** implementasyonu
- âœ… **GÃ¶rselleÅŸtirme** teknikleri
- âœ… **Model deÄŸerlendirmesi**

## ğŸ” Program Ã‡Ä±ktÄ±sÄ± Analizi

### BaÅŸarÄ± Kriterleri

**MÃ¼kemmel Performans (>%96)**:
- Loss deÄŸeri < 0.1
- Ã‡oÄŸu rakam doÄŸru tahmin edilir
- EÄŸitim sÃ¼reci stabil

**Ä°yi Performans (%90-96)**:
- Loss deÄŸeri 0.1-0.3 arasÄ±
- BazÄ± karmaÅŸÄ±k rakamlar karÄ±ÅŸabilir
- Genel trend azalan

**GeliÅŸtirilmesi Gereken (<90%)**:
- Loss deÄŸeri > 0.3
- Ã‡ok sayÄ±da yanlÄ±ÅŸ tahmin
- Model parametreleri gÃ¶zden geÃ§irilmeli

### YaygÄ±n Hatalar ve Ã‡Ã¶zÃ¼mleri

#### 1. DÃ¼ÅŸÃ¼k DoÄŸruluk
**Sebepler**:
- Yetersiz eÄŸitim sÃ¼resi
- YanlÄ±ÅŸ Ã¶ÄŸrenme oranÄ±
- AÅŸÄ±rÄ± basit model mimarisi

**Ã‡Ã¶zÃ¼mler**:
- Epoch sayÄ±sÄ±nÄ± artÄ±rÄ±n
- Learning rate'i ayarlayÄ±n (0.01, 0.0001)
- Daha fazla hidden layer ekleyin

#### 2. Overfitting
**Belirtiler**:
- EÄŸitim loss'u dÃ¼ÅŸer, test loss'u artar
- EÄŸitim doÄŸruluÄŸu yÃ¼ksek, test doÄŸruluÄŸu dÃ¼ÅŸÃ¼k

**Ã‡Ã¶zÃ¼mler**:
- Dropout katmanlarÄ± ekleyin
- L2 regularization kullanÄ±n
- Daha fazla eÄŸitim verisi

#### 3. Slow Training
**Sebepler**:
- CPU kullanÄ±mÄ±
- BÃ¼yÃ¼k batch size
- Veri yÃ¼kleme darboÄŸazÄ±

**Ã‡Ã¶zÃ¼mler**:
- GPU kullanÄ±mÄ±nÄ± etkinleÅŸtirin
- Batch size'Ä± optimize edin
- num_workers parametresi ayarlayÄ±n

## ğŸ› ï¸ GeliÅŸmiÅŸ Ã–zellikler

### GPU DesteÄŸi Ekleme
```python
# GPU kontrolÃ¼ ve model aktarÄ±mÄ±
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# Veriyi GPU'ya taÅŸÄ±ma
for images, labels in train_loader:
    images, labels = images.to(device), labels.to(device)
```

### Validation Set Ekleme
```python
# EÄŸitim verisini bÃ¶l
from torch.utils.data import random_split

train_size = int(0.8 * len(train_data))
val_size = len(train_data) - train_size
train_data, val_data = random_split(train_data, [train_size, val_size])
```

### Model Kaydetme
```python
# EÄŸitimli modeli kaydet
torch.save(model.state_dict(), 'mnist_model.pth')

# Modeli yÃ¼kle
model = SimpleANN()
model.load_state_dict(torch.load('mnist_model.pth'))
```

### Confusion Matrix
```python
from sklearn.metrics import confusion_matrix
import seaborn as sns

# TÃ¼m test verisi Ã¼zerinde tahmin
all_predicted = []
all_actual = []

with torch.no_grad():
    for images, labels in test_loader:
        outputs = model(images)
        _, predicted = torch.max(outputs, 1)
        all_predicted.extend(predicted.numpy())
        all_actual.extend(labels.numpy())

# Confusion matrix Ã§iz
cm = confusion_matrix(all_actual, all_predicted)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('MNIST Confusion Matrix')
plt.show()
```

## ğŸ“š Ä°leri Seviye Konular

### Model Ä°yileÅŸtirmeleri

#### 1. Daha Derin AÄŸ
```python
class DeepANN(nn.Module):
    def __init__(self):
        super(DeepANN, self).__init__()
        self.fc1 = nn.Linear(28*28, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, 128)
        self.fc4 = nn.Linear(128, 10)
        self.dropout = nn.Dropout(0.2)
        
    def forward(self, x):
        x = x.view(-1, 28*28)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = F.relu(self.fc3(x))
        x = self.fc4(x)
        return x
```

#### 2. Learning Rate Scheduling
```python
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)

for epoch in range(epochs):
    # ... eÄŸitim kodu
    scheduler.step()  # Her epoch sonunda learning rate gÃ¼ncelle
```

#### 3. Early Stopping
```python
best_val_loss = float('inf')
patience = 5
counter = 0

for epoch in range(epochs):
    # ... eÄŸitim ve validasyon
    
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        counter = 0
        torch.save(model.state_dict(), 'best_model.pth')
    else:
        counter += 1
        
    if counter >= patience:
        print("Early stopping!")
        break
```

## ğŸ“ˆ Alternatif YaklaÅŸÄ±mlar

### Convolutional Neural Network (CNN)
```python
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.dropout1 = nn.Dropout2d(0.25)
        self.dropout2 = nn.Dropout2d(0.5)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)
```

### Transfer Learning
```python
import torchvision.models as models

# Ã–nceden eÄŸitilmiÅŸ model kullanÄ±mÄ±
model = models.resnet18(pretrained=True)
model.fc = nn.Linear(model.fc.in_features, 10)
```

## ğŸ¯ Proje GeniÅŸletme Fikirleri

### BaÅŸlangÄ±Ã§ Seviyesi
1. **Batch size optimizasyonu**: FarklÄ± deÄŸerler deneyin
2. **Epoch sayÄ±sÄ±**: Daha uzun eÄŸitim sÃ¼releri
3. **Hyperparameter tuning**: Learning rate, hidden size

### Orta Seviye
1. **Validation set**: Model performansÄ±nÄ± daha iyi deÄŸerlendirin
2. **Data augmentation**: GÃ¶rÃ¼ntÃ¼ dÃ¶ndÃ¼rme, kaydÄ±rma
3. **Regularization**: L1, L2, Dropout teknikleri

### Ä°leri Seviye
1. **CNN implementasyonu**: KonvolÃ¼syonel katmanlar
2. **Ensemble methods**: Birden fazla model kombinasyonu
3. **Adversarial examples**: Modelin gÃ¼Ã§lÃ¼ yanlarÄ±nÄ± test etme

## ğŸ”— Ä°lgili Kaynaklar

### Resmi DokÃ¼mantasyon
- [PyTorch Documentation](https://pytorch.org/docs/)
- [MNIST Dataset](http://yann.lecun.com/exdb/mnist/)
- [torchvision.datasets](https://pytorch.org/vision/stable/datasets.html)

### EÄŸitim Materyalleri
- [Deep Learning with PyTorch](https://pytorch.org/tutorials/)
- [Neural Networks Explained](https://www.3blue1brown.com/topics/neural-networks)
- [CS231n Stanford Course](http://cs231n.stanford.edu/)

### Pratik Kaynaklar
- [PyTorch Examples](https://github.com/pytorch/examples)
- [MNIST Benchmarks](https://paperswithcode.com/sota/image-classification-on-mnist)

## ğŸ† BaÅŸarÄ± Metrikleri ve Benchmarklar

### MNIST State-of-the-Art
| Model | DoÄŸruluk | YÄ±l |
|-------|----------|-----|
| **Bu Proje (ANN)** | ~97% | 2024 |
| **LeNet-5** | 99.05% | 1998 |
| **Modern CNN** | >99.5% | 2010+ |
| **Ensemble Methods** | >99.8% | 2015+ |

### Performance KarÅŸÄ±laÅŸtÄ±rmasÄ±
```
Basit ANN:     ~97%    (Bu proje)
CNN:           ~99%    (Ã–nerilen upgrade)
ResNet:        ~99.5%  (Transfer learning)
Ensemble:      ~99.8%  (Ã‡oklu model)
```

---

**ğŸ”— Ä°lgili Projeler**: 
- `6_destekvektor_iris.py` - SVM SÄ±nÄ±flandÄ±rma
- `7_kMeans_iris.py` - Unsupervised Learning
- `8_kMeans_kDegerSecimi.py` - Hyperparameter Tuning

**ğŸ“§ Ä°letiÅŸim**: BTK AtÃ¶lye Multimedya GÃ¼venliÄŸi Projesi kapsamÄ±nda hazÄ±rlanmÄ±ÅŸtÄ±r.

**ğŸ·ï¸ Etiketler**: #DeepLearning #ANN #PyTorch #MNIST #ImageClassification #NeuralNetworks #MachineLearning #Python

**â­ Zorluk Seviyesi**: Orta | **â±ï¸ Tahmini SÃ¼re**: 45-60 dakika | **ğŸ‘¥ Hedef Kitle**: ML Ã¶ÄŸrencileri, deep learning baÅŸlangÄ±Ã§